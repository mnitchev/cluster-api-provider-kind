---
theme: ./presentation/assets/theme.json
author: Mario Nitchev
date: January 2, 2006
paging: Slide %d / %d
---

`.                                                                                                                     . `
`.                                                                                                                     . `
`.                                    ,@LLL@_                           ;LLLL@,                                        . `
`.                                  ,@LLLLLLLb                        ,@LLLLLLL@_                                      . `
`.                                 $LLLLLLLLLL@_                     $LLLLLLLLLLLh                                     . `
`.                               ;HLLLLLLLLLLLLLb    __,,,,,,,__   ,@LLLLLLLLLLLLL$w                                   . `
`.                             ,@LLL$P  ]LLLLLLLL@p@HLLLLLLLLLLL@h@LLLLLLLLL   T$LLL@,                                 . `
`.                           ,@LLLP^     5LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLP      ULLL@_                               . `
`.                          $LLLP        'LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLP         TLLLh                              . `
`.                        ;@LP^      ,;p@$LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLH@hw_      "UL@w                            . `
`.                      ,@LP      ,$$LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLh,      TL@_                          . `
`.                     @P"      ;@LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL@p      "Uh                         . `
`.                            ,@LLL5PP"""""U5LLLLLLLLLLLLLLLLLLLLLLLLLLLPP^"" ""%5LLL@,                                . `
`.                           ;LLLP            "ULLLLLLLLLLLLLLLLLLLLLP             TLLLp                               . `
`.                          ;LLLP                TLLLLLLLLLLLLLLLLLP                lLLLL                              . `
`.                          $LLP                   ULLLLLLLLLLLLLP                   ]LLL                              . `
`.                         !LLLL       Ppw_         ]LLLLLLLLLLLP                    ]LLLL                             . `
`.                         !LLLL       "P@@@@hw_     $LLLLLLLLLP     __________      ]LLLL                             . `
`.                          LLLL          _]@@@P     'LLLLLLLLL     @@@@@@@@@@@P     @LLL                              . `
`.                          ]LLL@      pP@@@PP"       LLLLLLLLL                     $LLLP                              . `
`.                           5LLLh     PP"            LLLLLLLL$                    $LLLP                               . `
`.                            ULLL@L                 /LLLLLLLLLL                 ,@LLLP                                . `
`.                             TLLLL@w_             ;$LLLLLLLLL$w              ,@LLLLP                                 . `
`.                               "5LLLL@w,_      ,;@LLLLLLLLLLLLL@w_       ,,@$LLLP^                                   . `
`.                                  "ULLLLL$@@@$LLLLLLLLLLLLLLLLLLLL$@@@@$LLLLLP"                                      . `
`.                                       "PPP$LLLLLLLLLLLLLLLLLLLLLLLLLPPPP"                                           . `
`.                                           ]LLLLLLLLLLLLLLLLLLLLLLLLP                                                . `
`.                                            ]LLLLLLLLLLLLLLLLLLLLLLP                                                 . `
`.                                             ^@LLLL""PPPPPPP]LLLLP^                                                  . `
`.                                                TPLh       _@$P"                                                     . `
`.                                                                                                                     . `

---

# Hi üëã ! I'm Mario

---

# Hi üëã ! I'm Mario

Here is some info about me:

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨
-   I have a Bachelor's Degree from Sofia University üìö

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨
-   I have a Bachelor's Degree from Sofia University üìö
-   I've worked for SAP for the past 5 years üìÜ

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨
-   I have a Bachelor's Degree from Sofia University üìö
-   I've worked for SAP for the past 5 years üìÜ
-   I love open source, Golang, Kubernetes and coding in general üíª

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨
-   I have a Bachelor's Degree from Sofia University üìö
-   I've worked for SAP for the past 5 years üìÜ
-   I love open source, Golang, Kubernetes and coding in general üíª
-   I play guitar and like playing video games üïπ

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨
-   I have a Bachelor's Degree from Sofia University üìö
-   I've worked for SAP for the past 5 years üìÜ
-   I love open source, Golang, Kubernetes and coding in general üíª
-   I play guitar and like playing video games üïπ
-   I have a cat named Tuna üêàüç£

---

# Hi üëã ! I'm Mario

Here is some info about me:

-   I'm from Sofia, Bulgaria üèû üáßüá¨
-   I have a Bachelor's Degree from Sofia University üìö
-   I've worked for SAP for the past 5 years üìÜ
-   I love open source, Golang, Kubernetes and coding in general üíª
-   I play guitar and like playing video games üïπ
-   I have a cat named Tuna üêàüç£
-   I am searching for a new job! üéâ

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created
-   When the CR is removed a cluster should be removed

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created
-   When the CR is removed a cluster should be removed
-   The CR should specify the KIND cluster name

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created
-   When the CR is removed a cluster should be removed
-   The CR should specify the KIND cluster name
-   The CR should specify and a ready boolean in the status

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created
-   When the CR is removed a cluster should be removed
-   The CR should specify the KIND cluster name
-   The CR should specify and a ready boolean in the status
-   Use finalizers to ensure deletion

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created
-   When the CR is removed a cluster should be removed
-   The CR should specify the KIND cluster name
-   The CR should specify and a ready boolean in the status
-   Use finalizers to ensure deletion
-   (OPTIONAL) Implement as much of the infrastructure provider specification as possible.

---

## The Hiring Task

```
Implement a KIND infrastructure provider which creates a KIND cluster
on your local machine when a KINDCluster custom resource is created
```

Requirements:

-   When the CR is created a cluster should be created
-   When the CR is removed a cluster should be removed
-   The CR should specify the KIND cluster name
-   The CR should specify and a ready boolean in the status
-   Use finalizers to ensure deletion
-   (OPTIONAL) Implement as much of the infrastructure provider specification as possible.
-   HINT: You do not need to implement any logic specific to machines

---

## The Hiring Task

```
~~~node presentation/diagrams/diagram-01.js

~~~
```

---

## The Hiring Task

Questions:

---

## The Hiring Task

Questions:

-   [ ] Is calling the kind cli the only option?

---

## The Hiring Task

Questions:

-   [ ] Is calling the kind cli the only option?
-   [ ] How hard is it to run on the cluster?

---

## The Hiring Task

Questions:

-   [ ] Is calling the kind cli the only option?
-   [ ] How hard is it to run on the cluster?
-   [ ] How should duplicate names be handled?

---

### Is calling the kind cli the only option?

---

### Is calling the kind cli the only option?

-   Using the cli is hard to test

---

### Is calling the kind cli the only option?

-   Using the cli is hard to test
-   Depends on parsing output which is unreliable

---

### Is calling the kind cli the only option?

-   Using the cli is hard to test
-   Depends on parsing output which is unreliable
-   Supporting all versions is hard

---

### Is calling the kind cli the only option?

-   Using the cli is hard to test
-   Depends on parsing output which is unreliable
-   Supporting all versions is hard
-   Luckily there is a go package!

---

### Is calling the kind cli the only option?

##### sigs.k8s.io/kind/pkg/cluster/provider.go

```go

// NewProvider returns a new provider based on the supplied options
func NewProvider(options ...ProviderOption) *Provider {
	// <redacted>
}
```

```go
// Create provisions and starts a kubernetes-in-docker cluster
func (p *Provider) Create(name string, options ...CreateOption) error {
	// <redacted>
}
```

```go
// Delete tears down a kubernetes-in-docker cluster
func (p *Provider) Delete(name, explicitKubeconfigPath string) error {
	// <redacted>
}
```

```go
// List returns a list of clusters for which nodes exist
func (p *Provider) List() ([]string, error) {
    // <redacted>
}

```

---

## The Hiring Task

##### State so far:

```
~~~node presentation/diagrams/diagram-02.js

~~~
```

---

## The Hiring Task

Questions:

-   [x] Is calling the kind cli the only option?
-   [ ] How hard is it to run on the cluster?
-   [ ] How should duplicate names be handled?

---

### How hard is it to run on the cluster?

---

### How hard is it to run on the cluster?

-   No more binary dependencies

---

### How hard is it to run on the cluster?

-   No more binary dependencies
-   Kind starts nodes with docker

---

### How hard is it to run on the cluster?

-   No more binary dependencies
-   Kind starts nodes with docker
-   The docker daemon would be unreachable

---

### How hard is it to run on the cluster?

-   No more binary dependencies
-   Kind starts nodes with docker
-   The docker daemon would be unreachable
-   Not exactly clear if there are any other dependencies

---

### How hard is it to run on the cluster?

###### Listing Clusters

##### sigs.k8s.io/kind/pkg/cluster/internal/providers/docker/provider.go

```go

// ListClusters is part of the providers.Provider interface
func (p *provider) ListClusters() ([]string, error) {
    cmd := exec.Command("docker",
        "ps",
        "-a", // show stopped nodes
        // filter for nodes with the cluster label
        "--filter", "label="+clusterLabelKey,
        // format to include the cluster name
        "--format", fmt.Sprintf(`{{.Label "%s"}}`, clusterLabelKey),
    )
    lines, err := exec.OutputLines(cmd)
    if err != nil {
        return nil, errors.Wrap(err, "failed to list clusters")
    }
    return sets.NewString(lines...).List(), nil
}


```

---

### How hard is it to run on the cluster?

-   ~~No more binary dependencies~~
-   Kind starts nodes with docker
-   The docker daemon would be unreachable
-   Not exactly clear if there are any other dependencies

---

### How hard is it to run on the cluster?

-   ~~No more binary dependencies~~ Docker is the only binary dependency!
-   Kind starts nodes with docker
-   The docker daemon would be unreachable
-   Not exactly clear if there are any other dependencies

---

### How hard is it to run on the cluster?

-   ~~No more binary dependencies~~ Docker is the only binary dependency!
-   Kind starts nodes with docker
-   The docker daemon would be unreachable
-   Not exactly clear if there are any other dependencies

Lets do a quick experiment!

---

### How hard is it to run on the cluster?

##### Adding docker to the image

```dockerfile
# syntax = docker/dockerfile:experimental
FROM golang:1.17 as builder
...
RUN curl -sLo docker-20.10.9.tgz https://download.docker.com/linux/static/stable/x86_64/docker-20.10.9.tgz && \
    tar xzvf docker-20.10.9.tgz
...

FROM gcr.io/distroless/static:latest
...
COPY --from=builder /workspace/docker/docker /usr/bin/docker
...
```

---

### How hard is it to run on the cluster?

##### Sharing the docker socket

kind-cluster-with-docker-sock.yaml

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
    - role: control-plane
      extraMounts:
          - hostPath: /var/run/docker.sock
            containerPath: /docker/docker.sock
```

---

### How hard is it to run on the cluster?

##### Mounting the socket on the deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
    name: controller-manager
    namespace: system
    labels:
        control-plane: controller-manager
spec:
    template:
        spec:
            volumes:
                - name: docker-sock
                  hostPath:
                  path: /docker/docker.sock
            containers:
                - command:
                      - /manager
                  args:
                      - --leader-elect
                  image: controller:latest
                  name: manager
                  volumeMounts:
                      - name: docker-sock
                        mountPath: /var/run/docker.sock
```

---

### How hard is it to run on the cluster?

##### Simple reconciler

```go
func (r *KindClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	log := log.FromContext(ctx)

	kindCluster := &infrastructurev1alpha3.KindCluster{}
	err := r.client.Get(ctx, req.NamespacedName, kindCluster)
	if err != nil {
		log.Error(err, "failed to get kind cluster")
		return ctrl.Result{}, err
	}

	err = r.clusterProvider.Create(kindCluster.Spec.Name, cluster.CreateWithKubeconfigPath("/junk/kubeconfig"))
	if err != nil {
		log.Error(err, "failed to create kind cluster")
		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
}
```

---

## The Hiring Task

##### State so far:

```
~~~node presentation/diagrams/diagram-03.js

~~~
```

---

## The Hiring Task

Questions:

-   [x] Is calling the kind cli the only option?
-   [x] How hard is it to run on the cluster?
-   [ ] How should duplicate names be handled?

---

### How should duplicate names be handled?

---

### How should duplicate names be handled?

-   Use name + namespace as the name

---

### How should duplicate names be handled?

-   ~~Use name + namespace as the name~~

---

### How should duplicate names be handled?

-   ~~Use name + namespace as the name~~
-   Use a webhook to validate the name

---

### How should duplicate names be handled?

-   ~~Use name + namespace as the name~~
-   ~~Use a webhook to validate the name~~

---

### How should duplicate names be handled?

-   ~~Use name + namespace as the name~~
-   ~~Use a webhook to validate the name~~
-   Use phases to detect foreign clusters

---

# Lets see it in action! üé¨üçø

---

# Implementing the Reconciler

```go
// KindClusterStatus defines the observed state of KindCluster
type KindClusterStatus struct {
	// Ready indicates if the cluster's control plane is running and ready to
	// be used
	//+kubebuilder:validation:Optional
	//+kubebuilder:default=false
	Ready bool `json:"ready"`
	// Phase indicates which phase the cluster creation is in
	//+kubebuilder:validation:Optional
	//+kubebuilder:default=Pending
	Phase ClusterPhase `json:"phase"`
}

```

---

# Implementing the Reconciler

```go
// KindClusterSpec defines the desired state of KindCluster
type KindClusterSpec struct {
	// Name is the name with which the actual kind cluster will be created. If
	// the name already exists the KindCluster will stay in the Pending phase
	// until the cluster is removed
	//+kubebuilder:validation:Required
	Name string `json:"name"`

	// ControlPlaneNodes specifies the number of control plane nodes for the
	// kind cluster
	//+optional
	ControlPlaneNodes int `json:"controlPlaneNodes"`

	// WorkerNodes specifies the number of worker nodes for the kind cluster
	//+optional
	WorkerNodes int `json:"workerNodes"`

	// ControlPlaneEndpoint is the host and port at which the cluster is
	// reachable. It will be set by the controller after the cluster has
	// reached the Created phase.
	//+optional
	ControlPlaneEndpoint APIEndpoint `json:"controlPlaneEndpoint"`
}

```

---

# Implementing the Reconciler

```go
func (r *KindClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	kindCluster, err := r.kindClusters.Get(ctx, req.NamespacedName)
	if k8serrors.IsNotFound(err) {
		logger.Info("KindCluster no longer exists")
		return ctrl.Result{}, nil
	}
	if err != nil {
		logger.Error(err, "failed to get KindCluster")
		return ctrl.Result{}, err
	}

	cluster, err := r.clusters.Get(ctx, kindCluster)
	if err != nil {
		logger.Error(err, "failed to get owner cluster")
		return ctrl.Result{}, err
	}

	if cluster == nil {
		logger.Info("KindCluster not owned by Cluster yet")
		return ctrl.Result{}, nil
	}

	if !kindCluster.DeletionTimestamp.IsZero() {
		return r.reconcileDeletion(ctx, logger, kindCluster)
	}

```

---

# Phase Pending

```go
	// Always update the status.
	// By default do not change the status - this is so we don't change the
	// status in the event of an error. In this case we should requeue the
	// event and try again.
	status := &kclusterv1.KindClusterStatus{
		Ready: kindCluster.Status.Ready,
		Phase: kindCluster.Status.Phase,
	}
	defer r.updateStatus(logger, status, kindCluster)
```

---

# Phase Pending

```go
	exists, err := r.clusterProvider.Exists(kindCluster.Spec.Name)
	if err != nil {
		logger.Error(err, "failed to check if kind cluster exists")
		return ctrl.Result{}, err
	}

	if !exists && createdCluster(kindCluster.Status.Phase) {
		logger.Info("cluster does not exist")
		status.Ready = false
		status.Phase = kclusterv1.ClusterPhasePending
		return ctrl.Result{}, nil
	}

```

```go
func createdCluster(phase kclusterv1.ClusterPhase) bool {
	return phase == kclusterv1.ClusterPhaseProvisioned || phase == kclusterv1.ClusterPhaseReady
}

```

---

# Phase Pending

```go
	if exists && kindCluster.Status.Phase == kclusterv1.ClusterPhasePending {
		existsErr := errors.New("cluster already exists")
		logger.Error(existsErr, "failed to reconcile")
		status.Ready = false
		status.Phase = kclusterv1.ClusterPhasePending

		return ctrl.Result{}, existsErr
	}

```

---

# Phase Provisioning

```go
	if kindCluster.Status.Phase == kclusterv1.ClusterPhasePending {
		err := r.kindClusters.AddFinalizer(ctx, kindCluster)
		if err != nil {
			logger.Error(err, "failed to add finalizer")
			return ctrl.Result{}, err
		}

		status.Ready = false
		status.Phase = kclusterv1.ClusterPhaseProvisioning

		go r.createCluster(logger, kindCluster)
		return ctrl.Result{Requeue: true}, nil
	}

```

---

# Phase Provisioned

```go

func (r *KindClusterReconciler) createCluster(logger logr.Logger, kindCluster *kclusterv1.KindCluster) {
	status := &kclusterv1.KindClusterStatus{
		Ready: false,
		Phase: kclusterv1.ClusterPhaseProvisioned,
	}
	defer r.updateStatus(logger, status, kindCluster)

	err := r.clusterProvider.Create(kindCluster.Spec.Name)
	if err != nil {
		status.Phase = kclusterv1.ClusterPhasePending
		logger.Error(err, "failed to create cluster")
		return
	}

	logger.Info("cluster created")
}

```

---

# Phase Ready

```go
	if kindCluster.Status.Phase == kclusterv1.ClusterPhaseProvisioned {
		logger.Info("cluster created")
		err = r.setControlPlaneEndpoint(ctx, logger, kindCluster)
		if err != nil {
			logger.Error(err, "failed to set control plane endpoint")
			return ctrl.Result{}, err
		}

		status.Ready = true
		status.Phase = kclusterv1.ClusterPhaseReady

		return ctrl.Result{}, nil
	}

```

---

# Phase Deleting

```go
	status := &kclusterv1.KindClusterStatus{
		Ready: false,
		Phase: kclusterv1.ClusterPhaseDeleting,
	}
	r.updateStatus(logger, status, kindCluster)

	err := r.clusterProvider.Delete(kindCluster.Spec.Name)
	if err != nil {
		logger.Error(err, "failed to delete kind cluster")
		return ctrl.Result{}, err
	}

	err = r.kindClusters.RemoveFinalizer(ctx, kindCluster)
	if err != nil {
		logger.Error(err, "failed to remove finalizer")
		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
```

---

## Testing Strategy

---

## Testing Strategy

-   All of the code was test driven

---

## Testing Strategy

-   All of the code was test driven
-   To make this possible Environment Tests use was limited

---

## Testing Strategy

-   All of the code was test driven
-   To make this possible Environment Tests use was limited
-   Instead just do regular unit tests for te reconciller

---

## Testing Strategy

-   All of the code was test driven
-   To make this possible Environment Tests use was limited
-   Instead just do regular unit tests for te reconciller
-   Add integration tests for the kind cluster Provider

---

## Testing Strategy

-   All of the code was test driven
-   To make this possible Environment Tests use was limited
-   Instead just do regular unit tests for te reconciller
-   Add integration tests for the kind cluster Provider
-   Add acceptance tests for the whole deployment

---

## Limitations / Improvements

---

## Limitations / Improvements

-   Very slow, especially under load

---

## Limitations / Improvements

-   Very slow, especially under load
-   No upgrade strategy for kind clusters

---

## Limitations / Improvements

-   Very slow, especially under load
-   No upgrade strategy for kind clusters
-   No failure messages / events on the resource

---

## Limitations / Improvements

-   Very slow, especially under load
-   No upgrade strategy for kind clusters
-   No failure messages / events on the resource
-   Bug with docker run exiting with 125 but still creating the cluster
